---
title: 'Итоговое экзаменационное задание'
subtitle: 'Проверка освоения курса R. Анализ взаимосвязи страхового рынка и финансово-экономических показателей стран мира'
author: "Mironkina Yuliya"
date: 'July 12, 2018'
output:
  html_document:
    theme: spacelab 
    keep_md: no
    number_sections: yes
    toc: yes
    df_print: kable
lang: ru-RU
editor_options:
  chunk_output_type: console
---


Признаюсь: это было совсем непросто! :)

Данные я решила взять связанные с моими научными интересами. 
Источник данных [The World Bank] (https://data.worldbank.org/indicator/)
В работе будет предпринята попытка *проанализировать взаимосвязи между развитием страхового рынка и различными социально-экономическими и финансовыми показателями развития стран мира*. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Подготовка к анализу данных
## Подключение необходимых пакетов

```{r loading_packages, message = FALSE}
library(tidyverse) # обработка данных, графики...
library(dplyr)
library(skimr)# описательные статистики
library(rio) # импорт фантастического количества форматов данных
library(lattice) #для графиков
library(DataExplorer) #для быстрого автоматического графического анализа
library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca,
library(dendextend) # визуализация дендрограмм
library(corrplot) # визуализация корреляций
library(broom) # метла превращает результаты оценивания моделей в таблички
library(naniar) # визуализация пропущенных значений
library(visdat) # визуализация пропущенных значений
library(patchwork) # удобное расположение графиков рядом
library(GGally) # больше готовых графиков
library(sjPlot) # ещё больше графиков
devtools::install_github("strengejacke/strengejacke")
library(lmtest) # диагностика линейных моделей
library(sjstats) # удобные мелкие функции для работы с моделями
 library(sandwich) # оценка Var для гетероскедастичности
library(AER) # работа с инструментальными переменными
library(huxtable) # красивые таблички в html, tex
library(stargazer) # красивые таблички в html, tex
library(texreg) # и снова красивые таблички в html, tex :)
library(estimatr) # модели с робастными ошибками
library(sandwich) # оценка Var для гетероскедастичности
library(caret) # пакет для подбора параметров разных моделей
library(FFTrees) # быстрые деревья
library(margins) # для подсчёта предельных эффектов
library(rpart.plot) # для картинок деревьев
library(plotROC) # визуализация ROC-кривой
library(ggeffects) # графики для предельных эффектов
library(MLmetrics) # метрики качества
library(ranger) # случайный лес
library(elasticnet) # LASSO
library(latex2exp) # формулы в подписях к графику
library(distances) # расчет различных расстояний
library(texreg) # таблички со сравнением моделей
library(vcd)
```

## Импорт данных из Excel и описательный анализ
```{r}
data <- import('Data_R.xlsx')
glimpse(data)
```
Видим, что имеем набор из наблюдений 59 стран по 21 переменным.
Из них 4 переменные восприняты как текстовые (две из них - Country и Continents таковыми, действительно, являются, первую так и оставим как названия стран, вторую переведем в факторную), и еще две (Life_is_more и Y_net_trade) необходимо перевести в факторные.

```{r}
data <- mutate(data, Continents = factor(Continents), Life_is_more = factor(Life_is_more), Y_net_trade = factor(Y_net_trade))
skim(data)
```

Теперь все в порядке - у нас 59 стран, по ним по 3 факторных переменных и по 17 числовых. Пропусков нет.
Опишем переменные, участвующие в исследовании:

```{r}
data_description <- import('Data_R_description.xlsx')
knitr::kable(data_description)
```

Посмотреть на набор данных можно с помощью различных функций. 

```{r}
glimpse(data) # Показывает все значения переменных в строчку (первые наблюдения)
head(data) # Показывает "шапку" таблицы данных - первые 6 строк таблицы
tail(data) # Показывает низ таблицы данных - послдние 6 строк таблицы
str(data) # показывает структуру данных и по несколько значен й для каждой переменной
# create_report(data) создает целый краткий отчет по данным "Data Profiling Report" - все дискриптивные статистики, гистограммы, корреляционная матрица, главные компоненты...  
```

Видим, что переменная "Insurance_dencity", Плотность страхования", которая будет зависимой затем в регрессиях, имеет асимметричное распределение. И, т.к. она является денежной, прологарифмируем ее и посмотрим гистограмму.

```{r}
data <- mutate(data, lnY = log(Insurance_dencity))
glimpse(data)
qplot(data = data, x = lnY, bins = 8) + labs(x = 'Логарифм плотности страхования (в $))', title = 'Гистограмма логарифма плотности страхования') 
```

# Исследование факторных признаков

## Описание факторных переменных 

```{r}
table(data$Continents)
table(data$Life_is_more)
table(data$Y_net_trade)
```

Видим, что переменная Continents (Континенты) имеет 6 уровней, в выборке представлены 9 стран из Африки, 14- из Америки, 12 из Азии, 9 из Среднего и Ближнего Востока, 14 - из Европы и 1 срана (Австралия) представляет Океанию.
В выборке 21 страна имеет преобладание страхования жизни в общем объеме премий и 38 - характеризуется большим объем премий по не-жизни.
У 31 страны преобладает импорт товаров и услуг, у 28 - экспорт (они больше продают, переменная Y_net_trade="positiv").

Посмотрим, как связаны категориальные переменные между собой.

## Таблицы сопряженности и проверка зависимости
```{r}
table1 <- table(data$Continents, data$Life_is_more)
table1 # таблица сопряженности континентов и преобладания страхования жизни
chisq.test(table1)
```

Видим, что преобладание страхования жизни существенно разнится по континентам - в Америке, Африке и на Востоке больше распространено страхование не-жизни, в Азии - страхование жизни. В Европе в среднем половина стран имеет преобладание не-жизни. Хи-квадрат-тест показывает статистическую значимость этих отличий (отвергает гипотезу о независимости переменных на любом разумном уровне значимости).

```{r}
table2 <- table(data$Continents, data$Y_net_trade)
table2 # таблица сопряженности континентов и преобладания продажи товаров и услуг 
chisq.test(table2)
table3 <- table(data$Y_net_trade, data$Life_is_more)
table3 # таблица сопряженности преобладания продажи товаров и услуг и преобладания страхования жизни
chisq.test(table3)
```

Две другие пары факторных переминных демонстрируют независимость между собой согласно критерию хи-квадрат.
Построим мозаичные графики, наглядно демонстрирующие полученные выводы

```{r}
mosaic(data = data, ~ Continents + Life_is_more, shade = TRUE, legend = TRUE)
mosaic(data = data, ~ Continents + Y_net_trade, shade = TRUE, legend = TRUE)
```

# Статистический анализ количественных признаков
## Диаграммы рассеивания и скрипичные графики
```{r}

splom(data[c(4, 10, 12, 13)])
ggplot(data = data) +   geom_violin(aes(x = Continents, y = Insurance_dencity)) +  labs(x = 'Континенты', y = 'Плотность страхования', title = 'Распределение плотности страхования по континентам')
ggplot(data = data) +   geom_violin(aes(x = Continents, y = Insurance_dencity)) +  facet_grid(Life_is_more ~ .) +  labs(x = 'Континенты', y = 'Плотность страхования', title = 'Распределение плотности страхования по континентам в зависимости от наличия преобладания страхования жизни (да - нижний график)')
ggplot(data = data) + geom_point(aes(x = GDP_per_capita, y = lnY)) +  geom_hline(aes(yintercept = mean(lnY))) +                                     labs(x = 'ВВП на душу населения ($)', y = 'Логарифм плотности страхования ($)', title = 'Зависимость логарифма плотности страхования от ВВП страны на душу населения', subtitle = 'Прямая - средняя логарифма плотности страхования по странам ', caption = 'Source: The World Bank')
```

Диаграммы дают различную интересную информацию относительно распределения плотности страхования по континентам, зависимости от ВВП (которая явно имеет нелинейную зависимость), различия в странах, где преобладает страхование жизни и не-жизни и т.п. 

## Корреляционный анализ

Отберем основные количественные переменные и проведем корреляционный анализ.

```{r}
data_num <- select(data, Insurance_dencity:Life_nonlife, Life, Net_trade_per_capita)
glimpse(data_num)
data_cor <- cor(data_num, use = 'pairwise.complete.obs')
corrplot(data_cor, method = 'color', type = 'upper')
corrplot(data_cor, method = 'pie', type = 'lower')
corrplot.mixed(data_cor)
```

Видим, что зависимая переменная (плотность страхования) наиболее тесно положительно связана с объемом премий по страхованию жизни и с ВВП на душу населения, отрицательно - с инфляцией и безработицей (все очень логично)

# Кластерный анализ 

## Масштабирование переменных

Для того, чтобы сравнивать переменные с разными единицами измерения, их масштабируют:
вычитают среднее и делят на оценку стандартного отклонения.

\[
x_i^* = (x_i - \bar x) / \hat \sigma_x,
\]
где $\bar x = \frac{x_1 + x_2 + \ldots + x_n}{n}$,
а
\[
\hat\sigma_x = \frac{(x_1 - \bar x)^2 + \ldots + (x_n - \bar x)^2}{n-1}.
\]

Отмасштабируем все числовые данные с помощью встроенной функции `scale()`.
Результат сохраним в таблице `data_stand`.

```{r}
data_stand <- mutate_if(data_num, is.numeric, ~ as.vector(scale(.)))
str(data_stand)
```

Дополнение в виде функции `as.vector` нужно потому, что функция `scale` возвращает матрицу, а каждый столбец должен быть вектором :)

Посмотрев на описательные статистики, видим, что стало со средним значением и стандартным отклонением переменных - среднее почти 0, ско=1.

## Выбор оптимального числа кластеров

Попробуем понять, сколько кластеров брать оптимально?
Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.

```{r}
g1 <- fviz_nbclust(data_stand, kmeans, method = 'wss') +
  labs(subtitle = 'Elbow method')
g1

g2 <- fviz_nbclust(data_stand, kmeans, method = 'silhouette') +
  labs(subtitle = 'Silhouette method')
g2

g3 <- fviz_nbclust(data_stand, kmeans, method = 'gap_stat') +
  labs(subtitle = 'Gap statistic method')
g3
```

С помощью хитрого пакета Томаса (`thomasp85`) располагать графики легко 
И очень красиво и информативно!

```{r}
g1 + (g2 / g3)
```

Итак, все три метода указывают на оптимальное число кластеров, равное 2 (или 5).
Уточним число кластеров построением дендрограммы с помощью иерархического кластерного анализа.

## Иерархическая кластеризация

Отличный способ разбить данные на группы — иерархическая кластеризация. Она не требует знания о числе кластеров и позволяет хорошо увидеть внутреннюю структуру данных и поределить оптимальное число кластеров.
В отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её.
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.

```{r}
data_dist <- dist(data_stand, method = 'euclidian')
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(data_dist)
```

Полученную матрицу расстояний можно передать функции `hclust()`, которая кластеризует данные.
Однако в пакете `factoextra` есть функция `hcut()`, которая работает с исходными данными.
Будем использовать её и попросим выделить пять кластеров в аргументе `k`.

```{r}
data_hcl <- hcut(data_stand, k = 5) 
```

С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендрограмму красивее.

```{r}
fviz_dend(data_hcl,
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

Дендрограмма показывает наличие двух стран, существенно отличающихся от остальных стран и выделяющихся даже в отдельные кластеры - это 9 страна, Колумбия - видимо, за счет резко отличающегося от всех числа коммерческих отделений банков и 44 - Сингапур, обладающая несколькими отличными харакетристиками. Поэтому деление на 2 кластера является оптимальным - между этими группами наибольшее расстояние.

Выявленные кластеры можно добавить к исходным данным!
```{r}
data_plus2 <- mutate(data, cluster = data_hcl$cluster)
glimpse(data_plus2)
```

Иерархичская кластеризация полезна и для визуализаций корреляций.
Если в функции `corrplot()` из одноимённого пакета указать аргумента `order = hclust`,
то мы получим сгруппированные по кластерам переменные.
Для красоты добавим ещё один аргумент — `addrect = ...`.
Он обведёт прямоугольниками указанное число кластеров.

```{r}
data_cor <- cor(data_stand)
corrplot(data_cor, order = 'hclust', addrect = 2)
corrplot(data_cor, order = 'hclust', addrect = 5)
```

Как раз последняя корреляционная матрица и показывает, по каким признакам сущетсвенно отличаются Колумбия - это число коммерческих банков, и Сингапур - показатели образования инфляции и безработицы в ней одни из самых низких.

## Кластеризация методом k-средних 

Выполним кластеризацию методом k-средних с помощью функции `kmeans`.

В качестве аргументов укажем отмасштабированные данные `data_stand` и количество кластеров `centers`.
Оптимальное количество кластеров согласно всем предыдущим исследованиям равно 2.
Сохраним результат этого действия в список `k_means_data`.

```{r}
set.seed(13)
k_means_data <- kmeans(data_stand, centers = 2)
k_means_data
```

В первый кластер попало 46 стран, во воторой - 13.

Посмотрим на содержимое списка `k_means_data` командой `attributes()`.
```{r}
attributes(k_means_data)
```

Помним также, что в Rstudio можно кликнуть на лупу справа от `k_means_protein` во вкладке `Environment`.

Теперь, зная, что искать, мы можем посмотреть, например,
на координаты центра кластеров или количество объектов в каждом из них.

```{r}
k_means_data$centers
k_means_data$cluster
k_means_data$size
```

Другой способ структурировать вывод `kmeans` — использовать команду `tidy` из пакета `broom`.

```{r}
tidy(k_means_data)
```

Первые девять неназванных переменных — центры кластеров по каждой переменной.

Осталось только визуализировать результаты!
Для этого будем использовать команду `fviz_cluster()` из пакета `factoextra`.
Её аргументы — результат кластеризации `k_means_data`,
исходные данные и ещё куча настроек вроде размера точек и цвета наблюдений-выбросов.
Мы только попросим выделять цветом кластеры по их границам и укажем аргумент `ellipse.type = 'convex'`.

```{r}
fviz_cluster(object = k_means_data, data = data_stand,
             ellipse.type = 'convex')
```

Видим, что кластеры получились вполне различимы в простраснтве признаков и не пересекаются. 
Метки кластерам легко добавить к исходным данным:

```{r}
data_plus <- mutate(data, cluster = k_means_data$cluster)
glimpse(data_plus)
```

# Построение линейных множественных регрессий зависимости плотности страхования от основных макроэкономических индикаторов развития стран

Итак, за зависимую переменную принимаем логарифм плотности страхования (предпосылка использования классической линейной регрессии - нормальность зависимой переменной) от разных наборов признаков. Для этого построим длинную и короткую регрессии.

## Длинная регрессия

Сначала построим длинную регрессию с самыми коррелированными факторными признаками - ВВП на душу населения, премиями по страхованию жизни на душу населения, долей страхования жизни в общем объеме премий, широкой денежной массой, занятостью в сфере услуг, плотностью населения, продажей товаров и услуг, безработицей, экспортом добавим в качестве дамми факторные переменные континента и преобладания страхования жизни

```{r}
glimpse(data)
model_long1 <- lm(data = data, lnY ~ GDP_per_capita + Life + Life_nonlife + Broad_money + Employment_services + Population_density + Exports + Net_trade_per_capita + Unemployment + Continents +  Life_is_more )
```

С помощью команды `summary()` посмотрим на описание модели.

```{r}
summary(model_long1)
```

Видим, что, как и можно было ожидать из матрицы корреляций, значимых переменных не так много. 

Построим регрессию, оставив только значимые переменные - регрессия будет с ВВП на душу населения,  долей страхования жизни в общем объеме премий, широкой денежной массой, занятостью в сфере услуг, экспортом и безработицей

```{r}
model_long2 <- lm(data = data, lnY ~ GDP_per_capita +  Life_nonlife + Broad_money + Employment_services +  Exports + Unemployment)
summary(model_long2)
```

Исключим еще одну ставшую незначимой переменную - экспорт

```{r}
model_long <- lm(data = data, lnY ~ GDP_per_capita +  Life_nonlife + Broad_money + Employment_services +  Unemployment)
summary(model_long)
```

Теперь все коэффициенты регрессии значимы, по крайней мере на уровне значимости 0,1, эту регрессию и назовем длинной.

Другой способ посмотреть на описание модели — воспользоваться функциями пакета `broom`.
Оценки коэффциентов, стандартные ошибки, p-значения выводит команда `tidy()`:
```{r}
tidy(model_long)
```

А функция `glance()` покажет общие характеристики модели, для которых достаточно одной строчки, — коэффциент детерминации, значение лог-функции правдоподобия, AIC, BIC...

```{r}
glance(model_long)

```
Если из общей таблицы описания модели нужна только информация о коэффициентах, то поможет команда `coeftest()` из пакета `lmtest()`.
```{r}
coeftest(model_long)
```

Посмотрим модель подробнее

```{r}
ggnostic(model = model_long)
```

В модели, конечно, есть выбросы, но мы это уже оценили на этапе кластерного анализа.

Итак, регрессионный анализ позволил нам получить следующее уравнение зависимости логарфима плотности страхования от ВВП на душу населения, доли страхования жизни в общем объеме премий, широкой денежной массы, занятости в сфере услуг и безработицы:

\[
\hat(lnY) = 1.16 + 0.000053GDP_per_capita +  1.159Life_nonlife + 0.00467Broad_money + 0.0378Employment_services +  0.00385Unemployment
\],

чnо позволяет количественно оценить влияние каждого фактора на плотность страхования/
Модель значима, объясняет 82,8% дисперсии зависимой переменной, имеет все значимые коэффициенты регресси вполне может использоваться для практических выводов.

Нарисуем линию регрессии для модели, задав по оси Х самый значимый факторный признак - ВВП на душу населения.
Для этого к диаграмме рассеяния добавим дополнительный слой `geom_smooth()`.
Внутри него за линейную регрессию отвечает `method = 'lm'`.

```{r}
ggplot(data = data, aes(x = GDP_per_capita, y=lnY)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

## Короткая регрессия 

Теперь построим короткую регрессию с самыми значимыми регрессорами  - ВВП на душу населения, долей страхования жизни в общем объеме премий и занятостью в сфере услуг  

```{r}
model_short <- lm(data = data, lnY ~ GDP_per_capita +  Life_nonlife + Employment_services)
summary(model_short)
tidy(model_short)
glance(model_short)
```

Здесь, конечно же, все коэффициенты регрессии значимы, и коэффициент детерминации (скоррективроанный) упал не так сильно - до 79,6%.

## Сравнение длинной и короткой регрессий

Сравним длинную и короткую модели.Если есть две вложенных модели, то есть одна является частным случаем другой, то можно провести тест Вальда, чтобы выбрать одну из них.

$H_0$: верна ограниченная (короткая) модель;

$H_1$: верная неограниченная (длинная) модель;
```{r}
waldtest( model_short, model_long)
```

p-value=0,045 говорит, что на уровне значимости 0,05 гипотезу о принятии короткой регресси надо отвергнуть в пользу длинной (хотя на другом уровне значимости она не будет отвергнута, что говорит о не таком уж и плохом ее качестве, как мы и отмечали уже).
Поэтому длинная регрессия лучше.

Больше, к сожалению, не успеваю, увлеклась сильно началом (хотя знаю и разобралась, как дальше делать :( )

Но итак исследование получилось очень интересное и информативное! 

# Спасибо за курс!

## Ура! :)
