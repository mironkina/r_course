---
title: 'Итоговое экзаменационное задание'
subtitle: 'Проверка освоения курса R. Анализ взаимосвязи страхового рынка и финансово-экономических показателей стран мира'
author: "Mironkina Yuliya"
date: 'July 12, 2018'
output:
  html_document:
    theme: spacelab 
    keep_md: no
    number_sections: yes
    toc: yes
    code_folding: hide
    df_print: kable
lang: ru-RU
editor_options:
  chunk_output_type: console
---


Признаюсь: это было совсем непросто! :)

Данные я решила взять связанные с моими научными интересами. 
Источник данных [The World Bank] (https://data.worldbank.org/indicator/)
В работе будет предпринята попытка *проанализировать взаимосвязи между развитием страхового рынка и различными социально-экономическими и финансовыми показателями развития стран мира*. 

Шаманское заклинание для настройки глобальных опций отчёта:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Подготовка к анализу данных
## Подключение необходимых пакетов

```{r loading_packages, message = FALSE}
library(tidyverse) # обработка данных, графики...
library(dplyr)
library(skimr)# описательные статистики
library(rio) # импорт фантастического количества форматов данных
library(lattice) #для графиков
library(DataExplorer) #для быстрого автоматического графического анализа
library(cluster) # кластерный анализ
library(factoextra) # визуализации kmeans, pca,
library(dendextend) # визуализация дендрограмм
library(corrplot) # визуализация корреляций
library(broom) # метла превращает результаты оценивания моделей в таблички
library(naniar) # визуализация пропущенных значений
library(visdat) # визуализация пропущенных значений
library(patchwork) # удобное расположение графиков рядом
library(GGally) # больше готовых графиков
library(sjPlot) # ещё больше графиков
devtools::install_github("strengejacke/strengejacke")
library(lmtest) # диагностика линейных моделей
library(sjstats) # удобные мелкие функции для работы с моделями
 library(sandwich) # оценка Var для гетероскедастичности
library(AER) # работа с инструментальными переменными
library(huxtable) # красивые таблички в html, tex
library(stargazer) # красивые таблички в html, tex
library(texreg) # и снова красивые таблички в html, tex :)
library(estimatr) # модели с робастными ошибками
library(sandwich) # оценка Var для гетероскедастичности
library(caret) # пакет для подбора параметров разных моделей
library(FFTrees) # быстрые деревья
library(margins) # для подсчёта предельных эффектов
library(rpart.plot) # для картинок деревьев
library(plotROC) # визуализация ROC-кривой
library(ggeffects) # графики для предельных эффектов
library(MLmetrics) # метрики качества
library(ranger) # случайный лес
library(factoextra) # графики для кластеризации и pca
library(elasticnet) # LASSO
library(latex2exp) # формулы в подписях к графику
library(distances) # расчет различных расстояний
library(texreg) # таблички со сравнением моделей
library(vcd)
```

## Импорт данных из Excel и описательный анализ
```{r}
data <- import('Data_R.xlsx')
skim(data)
```
Видим, что имеем набор из наблюдений 59 стран по 22 переменным.
Из них 4 переменные восприняты как текстовые (две из них - Country и Continents таковыми, действительно, являются, первую так и оставим как названия стран, вторую переведем в факторную), и еще две (Life_is_more и Y_net_trade) необходимо перевести в факторные.

```{r}
data <- mutate(data, Continents = factor(Continents), Life_is_more = factor(Life_is_more), Y_net_trade = factor(Y_net_trade))
skim(data)
```

Теперь все в порядке - у нас 59 стран, по ним по 3 факторных переменных и по 18 числовых. Пропусков нет.
Опишем переменные, участвующие в исследовании:

```{r}
data_description <- import('Data_R_description.xlsx')
knitr::kable(data_description)
```

Посмотреть на набор данных можно с помощью различных функций. 

```{r}
glimpse(data)
head(data)
tail(data)
str(data)
create_report(data) # создаем целый краткий отчет по данным - все дискриптивные статистики, гистограммы, корреляционная матрица, главные компоненты...  
```

Видим, что переменная "Плотность страхования", которая будет зависимой затем в регрессиях, имеет асимметричное распределение. И, т.к. она является денежной, прологарифмируем ее и посмотрим гистограмму.

```{r}
data <- mutate(data, lnY = log(Insurance_dencity))
skim(data)
qplot(data = data, x = lnY, bins = 8) + labs(x = 'Логарифм плотности страхования (в $))', title = 'Гистограмма логарифма плотности страхования') 
```

# Исследование факторных признаков

## Описание факторных переменных 

```{r}
table(data$Continents)

table(data$Life_is_more)
table(data$Y_net_trade)
```


## Таблицы сопряженности и проверка зависимости
```{r}
table1 <- table(data$Continents, data$Life_is_more)
table1 # таблица сопряженности континентов и преобладания страхования жизни
chisq.test(table1)
table2 <- table(data$Continents, data$Y_net_trade)
table2 # таблица сопряженности континентов и преобладания продажи товаров и услуг 
chisq.test(table2)
table3 <- table(data$Y_net_trade, data$Life_is_more)
table3 # таблица сопряженности преобладания продажи товаров и услуг и преобладания страхования жизни
chisq.test(table3)
mosaic(data = data, ~ Continents + Life_is_more, shade = TRUE, legend = TRUE)
mosaic(data = data, ~ Continents + Y_net_trade, shade = TRUE, legend = TRUE)

```

# Статистический анализ количественных признаков
## Диаграммы рассеивания и скричпичные графики
```{r}

splom(data[c(4, 10, 12, 13)])
ggplot(data = data) +   geom_violin(aes(x = Continents, y = Insurance_dencity)) +  labs(x = 'Континенты', y = 'Плотность страхования', title = 'Распределение плотности страхования по континентам')
ggplot(data = data) +   geom_violin(aes(x = Continents, y = Insurance_dencity)) +  facet_grid(Life_is_more ~ .) +  labs(x = 'Континенты', y = 'Плотность страхования', title = 'Распределение плотности страхования по континентам в зависимости от наличия преобладания страхования жизни (да - нижний график)')

```


# корреляционный анализ

Отберем все количественные переменные и проведем корреляционный анализ.

```{r}
data_col <- select(data
```


# Кластеризация k-means

## Масштабирование переменных

Для того, чтобы сравнивать переменные с разными единицами измерения, их масштабируют:
вычитают среднее и делят на оценку стандартного отклонения.

\[
x_i^* = (x_i - \bar x) / \hat \sigma_x,
\]
где $\bar x = \frac{x_1 + x_2 + \ldots + x_n}{n}$, а
\[
\hat\sigma_x = \frac{(x_1 - \bar x)^2 + \ldots + (x_n - \bar x)^2}{n-1}.
\]

Отмасштабируем все числовые данные с помощью встроенной функции `scale()`.
Поскольку она может работать только с числами, первый столбец `Country` ей передавать не нужно.
Результат сохраним в таблице `data_stand`.

```{r}
data_stand <- mutate_if(data - is.factor(), is.numeric, ~ as.vector(scale(.)))
skim(data_stand)
```

Дополнение в виде функции `as.vector` нужно потому, что функция `scale` возвращает матрицу, а каждый столбец должен быть вектором :)

Посмотрев на описательные статистики, видим, что стало со средним значением и стандартным отклонением переменных - среднее почти 0, ско=1.

Выполним кластеризацию методом k-средних с помощью функции `kmeans`.
Название страны не используется для кластеризации, но нужно для меток на графиках.
Поэтому мы уберем столбец `Country` из набора данных и превратим его в метки строк.

В качестве аргументов укажем отмасштабированные данные `data_no_country` и количество кластеров `centers`.
Пока мы не знаем, каково оптимальное количество кластеров, поэтому предположим, что их три.
Сохраним результат этого действия в список `k_means_data`.

```{r}
data_no_country <- data_stand %>% column_to_rownames(var = 'Country')
skim(data_no_country)
glimpse(data_no_country)
set.seed(13)
k_means_data <- kmeans(data_no_country, centers = 3)
k_means_data
```

Посмотрим на содержимое списка `k_means_data` командой `attributes()`.
```{r}
attributes(k_means_data)
```

Помним также, что в Rstudio можно кликнуть на лупу справа от `k_means_protein` во вкладке `Environment`.

Теперь, зная, что искать, мы можем посмотреть, например,
на координаты центра кластеров или количество объектов в каждом из них.

```{r}
k_means_data$centers
k_means_data$cluster
k_means_data$size
```

Другой способ структурировать вывод `kmeans` — использовать команду `tidy` из пакета `broom`.

```{r}
tidy(k_means_data)
```

Первые девять неназванных переменных — центры кластеров по каждой переменной.

Осталось только визуализировать результаты!
Для этого будем использовать команду `fviz_cluster()` из пакета `factoextra`.
Её аргументы — результат кластеризации `k_means_data`,
исходные данные и ещё куча настроек вроде размера точек и цвета наблюдений-выбросов.
Мы только попросим выделять цветом кластеры по их границам и укажем аргумент `ellipse.type = 'convex'`.

```{r}
fviz_cluster(object = k_means_data, data = data_no_country,
             ellipse.type = 'convex')
```
Попробуем понять, сколько кластеров брать оптимально?
Один из способов сделать это — воспользоваться командой `fviz_nbclust` из пакета `factoextra`.

```{r}
g1 <- fviz_nbclust(data_no_country, kmeans, method = 'wss') +
  labs(subtitle = 'Elbow method')
g1

g2 <- fviz_nbclust(data_no_country, kmeans, method = 'silhouette') +
  labs(subtitle = 'Silhouette method')
g2

g3 <- fviz_nbclust(data_no_country, kmeans, method = 'gap_stat') +
  labs(subtitle = 'Gap statistic method')
g3
```

С помощью хитрого пакета Томаса располагать графики легко!
И очень красиво и информативно!

```{r}
(g1 + g2) / g3
g1 + g2 + g3
g1 + (g2 / g3)
```

Метки кластерам легко добавить к исходным данным:

```{r}
data_plus <- mutate(data, cluster = k_means_data$cluster)
glimpse(data_plus)
```

# Иерархическая кластеризация

Другой способ разбить данные на группы — иерархическая кластеризация.
Но, в отличие от метода k-средних, она работает с матрицей расстояний,
поэтому первым делом посчитаем её!
Для этого будем использовать функцию `dist()`.
Передадим ей стандартизированные данные и укажем явно, как считать расстояния с помощью аргумента `method`.

```{r}
data_dist <- dist(data_no_country, method = 'euclidian')
```

Расстояния тоже можно визуализировать!
Сделаем это командой `fviz_dist` из пакета `factoextra`.

```{r}
fviz_dist(data_dist)
```

Полученную матрицу расстояний можно передадать функции `hclust()`, которая кластеризует данные.
Однако в пакете `factoextra` есть функция `hcut()`, которая работает с исходными данными.
Будем использовать её и попросим выделить четыре кластера в аргументе `k`.

```{r}
datan_hcl <- hcut(data_no_country, k = 4)
```

С помощью функции `fviz_dend` визуализируем результат кластеризации.
Укажем несколько аргументов, чтобы сделать дендрограмму красивее.

```{r}
fviz_dend(data_hcl,
          cex = 0.5, # размер подписи
          color_labels_by_k = TRUE) # цвет подписей по группам
```

Выявленные кластеры можно добавить к исходным данным!
```{r}
data_plus2 <- mutate(data, cluster = data_hcl$cluster)
glimpse(data_plus2)
```

Иерархичская кластеризация полезна и для визуализаций корреляций.
Если в функции `corrplot()` из одноимённого пакета указать аргумента `order = hclust`,
то мы получим сгруппированные по кластерам переменные.
Для красоты добавим ещё один аргумент — `addrect = 3`.
Он обведёт прямоугольниками указанное число кластеров.

```{r}
data_cor <- cor(data_no_country)
corrplot(data_cor, order = 'hclust', addrect = 3)
```

Ура! :)
